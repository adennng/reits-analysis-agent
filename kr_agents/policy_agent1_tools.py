# kr_agents/policy_agent1_tools.py
"""
æ”¿ç­–æ–‡ä»¶Agent1ä¸“é—¨å·¥å…·é›†åˆ
ä¸ºæ”¿ç­–æ–‡ä»¶ä¸»æ§è°ƒåº¦å™¨Agentæä¾›ä¸¤ä¸ªæ ¸å¿ƒå·¥å…·ï¼š
1. æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†å·¥å…·
2. æ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå·¥å…·
"""

import sys
import os
import json
from typing import Dict, Any, List

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

try:
    from agents import function_tool
except ImportError:
    # For testing purposes, create mock decorator
    def function_tool(func):
        return func

# å¯¼å…¥é…ç½®å’Œå·¥å…·
from config.prompts import (
    POLICY_QUESTION_SPLITTING_PROMPT,
    POLICY_FINAL_ANSWER_GENERATION_PROMPT
)
from config.model_config import get_deepseek_v3_model

print("[PolicyAgent1Tools] å¼€å§‹åˆå§‹åŒ–æ”¿ç­–æ–‡ä»¶Agent1ä¸“é—¨å·¥å…·")

# ==================== ä¸“ä¸šåŒ–å·¥å…·ç±» ====================

class PolicyQuestionSplitter:
    """æ”¿ç­–æ–‡ä»¶ä¸“ç”¨é—®é¢˜æ‹†åˆ†å·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.prompt_template = POLICY_QUESTION_SPLITTING_PROMPT
        self.openai_client = None
        self._initialized = False
        print("[PolicyQuestionSplitter] æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†å·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_llm_client(self) -> Dict[str, Any]:
        """è®¾ç½®LLMå®¢æˆ·ç«¯"""
        try:
            if self.model is None:
                self.model = get_deepseek_v3_model()
                if self.model is None:
                    return {
                        "success": False,
                        "error": "æ— æ³•åˆ›å»ºdeepseek-v3æ¨¡å‹"
                    }
            
            # è·å–åº•å±‚çš„OpenAIå®¢æˆ·ç«¯
            if hasattr(self.model, 'openai_client'):
                self.openai_client = self.model.openai_client
            else:
                from config.model_config import MODEL_CONFIG
                from openai import OpenAI
                
                config = MODEL_CONFIG.get("ali", {}).get("deepseek-v3", {})
                if not config:
                    return {
                        "success": False,
                        "error": "deepseek-v3é…ç½®æœªæ‰¾åˆ°"
                    }
                
                self.openai_client = OpenAI(
                    api_key=config['api_key'],
                    base_url=config['base_url']
                )
            
            self._initialized = True
            print("[PolicyQuestionSplitter] LLMå®¢æˆ·ç«¯è®¾ç½®æˆåŠŸ")
            return {
                "success": True,
                "model": "deepseek-v3"
            }
            
        except Exception as e:
            error_msg = f"LLMå®¢æˆ·ç«¯è®¾ç½®å¤±è´¥: {str(e)}"
            print(f"[PolicyQuestionSplitter] {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _call_llm(self, prompt: str, max_tokens: int = 4096) -> Dict[str, Any]:
        """è°ƒç”¨LLMè·å–å“åº”"""
        try:
            if not self._initialized:
                setup_result = self._setup_llm_client()
                if not setup_result["success"]:
                    return {
                        "success": False,
                        "error": setup_result["error"]
                    }
            
            response = self.openai_client.chat.completions.create(
                model="deepseek-v3",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            return {
                "success": True,
                "response": raw_response
            }
            
        except Exception as e:
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"[PolicyQuestionSplitter] {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """è§£æJSONå“åº”"""
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                parsed = json.loads(response_text.strip())
                return {
                    "success": True,
                    "data": parsed
                }
            except json.JSONDecodeError:
                # å°è¯•æå–JSONå†…å®¹
                import re
                
                # æŸ¥æ‰¾JSONå—
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(0)
                    parsed = json.loads(json_text)
                    return {
                        "success": True,
                        "data": parsed
                    }
                else:
                    return {
                        "success": False,
                        "error": "å“åº”ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå†…å®¹"
                    }
                    
        except Exception as e:
            return {
                "success": False,
                "error": f"JSONè§£æå¤±è´¥: {str(e)}"
            }
    
    async def split(self, question: str) -> dict:
        """
        æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†
        
        Args:
            question: ç”¨æˆ·çš„æ”¿ç­–é—®é¢˜
            
        Returns:
            {
                "success": True,
                "questions": ["å­é—®é¢˜1", "å­é—®é¢˜2", ...],
                "analysis": "æ‹†åˆ†åˆ†æè¯´æ˜",
                "total_sub_questions": 2
            }
        """
        print(f"[PolicyQuestionSplitter] å¼€å§‹æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†")
        print(f"  é—®é¢˜: {question}")
        
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self.prompt_template.format(question=question)
            
            # è°ƒç”¨LLMè¿›è¡Œåˆ†æ
            llm_result = self._call_llm(prompt, max_tokens=4096)
            if not llm_result["success"]:
                return {
                    "success": False,
                    "error": f"LLMåˆ†æå¤±è´¥: {llm_result['error']}",
                    "questions": [question],  # fallbackåˆ°åŸé—®é¢˜
                    "analysis": "LLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸé—®é¢˜",
                    "total_sub_questions": 1
                }
            
            # è§£æJSONå“åº”
            parse_result = self._parse_json_response(llm_result["response"])
            if not parse_result["success"]:
                return {
                    "success": False,
                    "error": f"å“åº”è§£æå¤±è´¥: {parse_result['error']}",
                    "questions": [question],  # fallbackåˆ°åŸé—®é¢˜
                    "analysis": "JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨åŸé—®é¢˜",
                    "total_sub_questions": 1
                }
            
            result_data = parse_result["data"]
            
            # éªŒè¯å’Œæ ¼å¼åŒ–ç»“æœ
            questions = result_data.get("questions", [question])
            analysis = result_data.get("analysis", "")
            total_sub_questions = result_data.get("total_sub_questions", len(questions))
            
            print(f"[PolicyQuestionSplitter] æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†å®Œæˆï¼Œç”Ÿæˆ {len(questions)} ä¸ªå­é—®é¢˜")
            print(f"[PolicyQuestionSplitter] æ‹†åˆ†åˆ†æ: {analysis}")
            
            return {
                "success": True,
                "questions": questions,
                "analysis": analysis,
                "total_sub_questions": total_sub_questions
            }
            
        except Exception as e:
            error_msg = f"æ”¿ç­–æ–‡ä»¶é—®é¢˜æ‹†åˆ†å¼‚å¸¸: {str(e)}"
            print(f"[PolicyQuestionSplitter] {error_msg}")
            import traceback
            traceback.print_exc()
            
            return {
                "success": False,
                "error": error_msg,
                "questions": [question],  # fallbackåˆ°åŸé—®é¢˜
                "analysis": "æ‹†åˆ†è¿‡ç¨‹å¼‚å¸¸ï¼Œä½¿ç”¨åŸé—®é¢˜",
                "total_sub_questions": 1
            }

class PolicyFinalAnswerGenerator:
    """æ”¿ç­–æ–‡ä»¶ä¸“ç”¨æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.prompt_template = POLICY_FINAL_ANSWER_GENERATION_PROMPT
        self.openai_client = None
        self._initialized = False
        print("[PolicyFinalAnswerGenerator] æ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå·¥å…·åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_llm_client(self) -> Dict[str, Any]:
        """è®¾ç½®LLMå®¢æˆ·ç«¯"""
        try:
            if self.model is None:
                self.model = get_deepseek_v3_model()
                if self.model is None:
                    return {
                        "success": False,
                        "error": "æ— æ³•åˆ›å»ºdeepseek-v3æ¨¡å‹"
                    }
            
            # è·å–åº•å±‚çš„OpenAIå®¢æˆ·ç«¯
            if hasattr(self.model, 'openai_client'):
                self.openai_client = self.model.openai_client
            else:
                from config.model_config import MODEL_CONFIG
                from openai import OpenAI
                
                config = MODEL_CONFIG.get("ali", {}).get("deepseek-v3", {})
                if not config:
                    return {
                        "success": False,
                        "error": "deepseek-v3é…ç½®æœªæ‰¾åˆ°"
                    }
                
                self.openai_client = OpenAI(
                    api_key=config['api_key'],
                    base_url=config['base_url']
                )
            
            self._initialized = True
            print("[PolicyFinalAnswerGenerator] LLMå®¢æˆ·ç«¯è®¾ç½®æˆåŠŸ")
            return {
                "success": True,
                "model": "deepseek-v3"
            }
            
        except Exception as e:
            error_msg = f"LLMå®¢æˆ·ç«¯è®¾ç½®å¤±è´¥: {str(e)}"
            print(f"[PolicyFinalAnswerGenerator] {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _call_llm(self, prompt: str, max_tokens: int = 8192) -> Dict[str, Any]:
        """è°ƒç”¨LLMè·å–å“åº”"""
        try:
            if not self._initialized:
                setup_result = self._setup_llm_client()
                if not setup_result["success"]:
                    return {
                        "success": False,
                        "error": setup_result["error"]
                    }
            
            response = self.openai_client.chat.completions.create(
                model="deepseek-v3",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=max_tokens
            )
            
            raw_response = response.choices[0].message.content.strip()
            
            return {
                "success": True,
                "response": raw_response
            }
            
        except Exception as e:
            error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"[PolicyFinalAnswerGenerator] {error_msg}")
            return {
                "success": False,
                "error": error_msg
            }
    
    def _parse_text_response(self, response_text: str) -> Dict[str, Any]:
        """è§£ææ–‡æœ¬å“åº”ï¼ˆä¸å†æ˜¯JSONï¼‰"""
        try:
            # ç›´æ¥è¿”å›æ–‡æœ¬å†…å®¹ï¼Œä¸éœ€è¦JSONè§£æ
            if response_text and response_text.strip():
                return {
                    "success": True,
                    "text": response_text.strip()
                }
            else:
                return {
                    "success": False,
                    "error": "å“åº”å†…å®¹ä¸ºç©º"
                }
                    
        except Exception as e:
            return {
                "success": False,
                "error": f"æ–‡æœ¬è§£æå¤±è´¥: {str(e)}"
            }
    
    async def generate(self, original_question: str, agent2_result: dict) -> str:
        """
        æ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆç”Ÿæˆ - ç°åœ¨è¿”å›ç›´æ¥çš„ç”¨æˆ·å¯è¯»æ–‡æœ¬
        
        Args:
            original_question: ç”¨æˆ·çš„åŸå§‹æ”¿ç­–é—®é¢˜
            agent2_result: æ”¿ç­–æ–‡ä»¶Agent2è¿”å›çš„å®Œæ•´æ£€ç´¢ç»“æœ
            
        Returns:
            str: ç›´æ¥çš„ç”¨æˆ·å¯è¯»æ–‡æœ¬ï¼ŒåŒ…å«ç­”æ¡ˆå’Œæ ¼å¼åŒ–çš„å‚è€ƒæ–‡ä»¶åˆ—è¡¨
        """
        print(f"[PolicyFinalAnswerGenerator] å¼€å§‹ç”Ÿæˆæ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆ")
        print(f"  åŸå§‹é—®é¢˜: {original_question}")
        print(f"  Agent2ç»“æœæˆåŠŸæ•°: {agent2_result.get('successful_queries', 0)}")
        print(f"  Agent2ç»“æœå¤±è´¥æ•°: {agent2_result.get('failed_queries', 0)}")
        
        try:
            # å‡†å¤‡LLMæç¤ºè¯ï¼Œä¼ é€’å®Œæ•´çš„Agent2ç»“æœ
            import json as json_module
            prompt = self.prompt_template.format(
                original_question=original_question,
                agent2_result=json_module.dumps(agent2_result, ensure_ascii=False, indent=2)
            )
            
            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            llm_result = self._call_llm(prompt, max_tokens=8192)
            if not llm_result["success"]:
                # LLMè°ƒç”¨å¤±è´¥æ—¶çš„fallbackå¤„ç†
                return self._generate_fallback_text_answer(agent2_result, llm_result["error"])
            
            # è§£ææ–‡æœ¬å“åº”ï¼ˆä¸å†æ˜¯JSONï¼‰
            parse_result = self._parse_text_response(llm_result["response"])
            if not parse_result["success"]:
                # è§£æå¤±è´¥æ—¶ç”Ÿæˆå¤‡ç”¨ç­”æ¡ˆ
                return self._generate_fallback_text_answer(agent2_result, f"æ–‡æœ¬è§£æå¤±è´¥: {parse_result['error']}")
            
            # ç›´æ¥è¿”å›LLMç”Ÿæˆçš„è‡ªç„¶æ–‡æœ¬
            final_text = parse_result["text"]
            
            print("[PolicyFinalAnswerGenerator] æ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            print(f"  ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(final_text)}")
            
            return final_text
            
        except Exception as e:
            error_msg = f"æ”¿ç­–æ–‡ä»¶æœ€ç»ˆç­”æ¡ˆç”Ÿæˆå¼‚å¸¸: {str(e)}"
            print(f"[PolicyFinalAnswerGenerator] {error_msg}")
            import traceback
            traceback.print_exc()
            
            return self._generate_fallback_text_answer(agent2_result, error_msg)
    
    def _generate_fallback_text_answer(self, agent2_result: dict, error_msg: str) -> str:
        """ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„fallbackç­”æ¡ˆ"""
        print(f"[PolicyFinalAnswerGenerator] ç”Ÿæˆæ–‡æœ¬æ ¼å¼fallbackç­”æ¡ˆï¼Œé”™è¯¯: {error_msg}")
        
        # å°è¯•ä»Agent2ç»“æœä¸­æå–æœ‰ç”¨ä¿¡æ¯
        try:
            if agent2_result.get("results"):
                successful_results = [r for r in agent2_result["results"] if r.get("is_found")]
                if successful_results:
                    # æœ‰æˆåŠŸçš„ç»“æœï¼Œç»„åˆç­”æ¡ˆå’Œå‚è€ƒæ–‡ä»¶
                    answers = []
                    all_reference_files = []
                    
                    for result in successful_results:
                        if result.get("answer"):
                            answers.append(result["answer"])
                        if result.get("reference_files"):
                            all_reference_files.extend(result["reference_files"])
                    
                    if answers:
                        # ç»„åˆç­”æ¡ˆå†…å®¹
                        answer_text = "\n\n".join(answers)
                        
                        # æ ¼å¼åŒ–å‚è€ƒæ–‡ä»¶åˆ—è¡¨
                        reference_text = self._format_reference_files(all_reference_files)
                        
                        # ç»„åˆæœ€ç»ˆæ–‡æœ¬
                        if reference_text:
                            return f"{answer_text}\n\n{reference_text}"
                        else:
                            return answer_text
                    else:
                        return "å¾ˆæŠ±æ­‰ï¼Œæœªåœ¨æ”¿ç­–æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆã€‚"
                else:
                    return "å¾ˆæŠ±æ­‰ï¼Œæœªåœ¨æ”¿ç­–æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆã€‚"
            else:
                return "å¾ˆæŠ±æ­‰ï¼Œæœªåœ¨æ”¿ç­–æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆã€‚"
                
        except Exception as e:
            print(f"[PolicyFinalAnswerGenerator] fallbackå¤„ç†å¼‚å¸¸: {e}")
            return "å¾ˆæŠ±æ­‰ï¼Œæœªåœ¨æ”¿ç­–æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³ç­”æ¡ˆã€‚"
    
    def _format_reference_files(self, reference_files: List[dict]) -> str:
        """æ ¼å¼åŒ–å‚è€ƒæ–‡ä»¶åˆ—è¡¨ä¸ºæ–‡æœ¬"""
        if not reference_files:
            return ""
        
        # å»é‡å‚è€ƒæ–‡ä»¶
        unique_files = []
        seen_titles = set()
        
        for file_info in reference_files:
            if isinstance(file_info, dict):
                title = file_info.get("document_title", "")
                if title and title not in seen_titles:
                    unique_files.append(file_info)
                    seen_titles.add(title)
        
        if not unique_files:
            return ""
        
        # æ ¼å¼åŒ–ä¸ºç¼–å·åˆ—è¡¨
        reference_lines = ["å‚è€ƒæ–‡ä»¶ï¼š"]
        for i, file_info in enumerate(unique_files, 1):
            document_title = file_info.get("document_title", "æœªçŸ¥æ–‡ä»¶")
            publish_date = file_info.get("publish_date", "æœªçŸ¥æ—¥æœŸ")
            issuing_agency = file_info.get("issuing_agency", "æœªçŸ¥æœºæ„")
            website = file_info.get("website", "")
            
            # æ ¼å¼ï¼šåºå·. æ–‡æ¡£æ ‡é¢˜ (å‘å¸ƒæ—¥æœŸ, å‘å¸ƒæœºæ„, æ¥æº: ç½‘ç«™é“¾æ¥)
            line = f"{i}. {document_title} ({publish_date}, {issuing_agency}"
            if website:
                line += f", æ¥æº: {website}"
            line += ")"
            
            reference_lines.append(line)
        
        return "\n".join(reference_lines)
    
    def _generate_fallback_answer(self, agent2_result: dict, error_msg: str, raw_response: str = "") -> dict:
        """ç”Ÿæˆfallbackç­”æ¡ˆ - ä¿ç•™åŸæœ‰æ¥å£å…¼å®¹æ€§"""
        print(f"[PolicyFinalAnswerGenerator] ç”Ÿæˆfallbackç­”æ¡ˆï¼Œé”™è¯¯: {error_msg}")
        
        # å°è¯•ä»Agent2ç»“æœä¸­æå–æœ‰ç”¨ä¿¡æ¯
        fallback_answer = ""
        reference_files = []
        
        try:
            if agent2_result.get("results"):
                successful_results = [r for r in agent2_result["results"] if r.get("is_found")]
                if successful_results:
                    # æœ‰æˆåŠŸçš„ç»“æœï¼Œå°è¯•ç®€å•ç»„åˆ
                    answers = []
                    for result in successful_results:
                        if result.get("answer"):
                            answers.append(result["answer"])
                        if result.get("reference_files"):
                            reference_files.extend(result["reference_files"])
                    
                    if answers:
                        fallback_answer = "æ ¹æ®æ”¿ç­–æ–‡ä»¶æ£€ç´¢ç»“æœï¼š\n\n" + "\n\n".join(answers)
                    else:
                        fallback_answer = "æ£€ç´¢åˆ°ç›¸å…³æ”¿ç­–å†…å®¹ï¼Œä½†ç­”æ¡ˆæ•´åˆå¤±è´¥"
                else:
                    fallback_answer = "æœªåœ¨æ”¿ç­–æ–‡ä»¶ä¸­æ‰¾åˆ°ç›¸å…³è§„å®š"
            else:
                fallback_answer = "æ”¿ç­–æ–‡ä»¶æ£€ç´¢æœªè¿”å›æœ‰æ•ˆç»“æœ"
                
        except Exception as e:
            print(f"[PolicyFinalAnswerGenerator] fallbackå¤„ç†å¼‚å¸¸: {e}")
            fallback_answer = f"æ”¿ç­–æ–‡ä»¶ç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼š{error_msg}"
        
        # å¦‚æœæœ‰raw_responseä¸”ä¸ä¸ºç©ºï¼Œä¼˜å…ˆä½¿ç”¨
        if raw_response and raw_response.strip():
            fallback_answer = raw_response
        
        # å»é‡å‚è€ƒæ–‡ä»¶
        unique_files = []
        seen_titles = set()
        for file_info in reference_files:
            if isinstance(file_info, dict):
                title = file_info.get("document_title", "")
                if title and title not in seen_titles:
                    unique_files.append(file_info)
                    seen_titles.add(title)
        
        return {
            "final_answer": fallback_answer,
            "reference_files": unique_files
        }
    

# ==================== å¯¼å‡ºæ¥å£ ====================

# å¯¼å‡ºæ¥å£
__all__ = [
    'PolicyQuestionSplitter',
    'PolicyFinalAnswerGenerator'
]

if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ”¿ç­–æ–‡ä»¶Agent1å·¥å…·")
    
    # ç®€å•çš„åˆå§‹åŒ–æµ‹è¯•
    model = get_deepseek_v3_model()
    
    print("\n1. æµ‹è¯•PolicyQuestionSplitter")
    splitter = PolicyQuestionSplitter(model)
    print("âœ… PolicyQuestionSplitteråˆå§‹åŒ–æˆåŠŸ")
    
    print("\n2. æµ‹è¯•PolicyFinalAnswerGenerator")
    generator = PolicyFinalAnswerGenerator(model)
    print("âœ… PolicyFinalAnswerGeneratoråˆå§‹åŒ–æˆåŠŸ")
    
    print("\nğŸ‰ æ”¿ç­–æ–‡ä»¶Agent1å·¥å…·æµ‹è¯•å®Œæˆï¼")